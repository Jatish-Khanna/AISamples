{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeHfH97cXo/MK4SlLpqGy5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatish-Khanna/AISamples/blob/main/Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6e750b",
        "outputId": "ccfd2d56-921b-4f43-ceea-ed48e0da823e"
      },
      "source": [
        "import tiktoken\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import hashlib\n",
        "from enum import Enum\n",
        "\n",
        "class ContextStrategy(Enum):\n",
        "    SLIDING_WINDOW = \"sliding_window\"\n",
        "    SUMMARIZATION = \"summarization\"\n",
        "    SEMANTIC_COMPRESSION = \"semantic_compression\"\n",
        "    HIERARCHICAL = \"hierarchical\"\n",
        "\n",
        "@dataclass\n",
        "class MessageMetadata:\n",
        "    timestamp: datetime\n",
        "    token_count: int\n",
        "    importance_score: float = 0.0\n",
        "    message_type: str = \"normal\"  # system, user, assistant, summary\n",
        "    thread_id: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class ConversationMemory:\n",
        "    messages: List[Dict[str, str]] = field(default_factory=list)\n",
        "    metadata: List[MessageMetadata] = field(default_factory=list)\n",
        "    summaries: List[Dict[str, str]] = field(default_factory=list)\n",
        "    persona: str = \"assistant\"\n",
        "    total_tokens: int = 0\n",
        "    created_at: datetime = field(default_factory=datetime.now)\n",
        "    last_accessed: datetime = field(default_factory=datetime.now)\n",
        "    context_strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW\n",
        "\n",
        "class AdvancedContextManager:\n",
        "    def __init__(self, max_tokens: int = 4000, strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.strategy = strategy\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "        self.summary_threshold = max_tokens * 0.7  # Summarize when 70% full\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Count tokens in text\"\"\"\n",
        "        return len(self.encoding.encode(text))\n",
        "\n",
        "    def calculate_importance_score(self, message: Dict[str, str], context: ConversationMemory) -> float:\n",
        "        \"\"\"Calculate importance score for message prioritization\"\"\"\n",
        "        content = message.get(\"content\", \"\")\n",
        "        role = message.get(\"role\", \"\")\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        # Role-based scoring\n",
        "        if role == \"system\":\n",
        "            score += 10.0\n",
        "        elif role == \"user\":\n",
        "            score += 5.0\n",
        "        elif role == \"assistant\":\n",
        "            score += 3.0\n",
        "\n",
        "        # Content-based scoring\n",
        "        importance_keywords = [\n",
        "            \"important\", \"remember\", \"key\", \"critical\", \"essential\",\n",
        "            \"problem\", \"solution\", \"error\", \"bug\", \"issue\"\n",
        "        ]\n",
        "\n",
        "        for keyword in importance_keywords:\n",
        "            if keyword.lower() in content.lower():\n",
        "                score += 2.0\n",
        "\n",
        "        # Length-based scoring (longer messages often more important)\n",
        "        score += min(len(content) / 100, 5.0)\n",
        "\n",
        "        # Recency boost (recent messages slightly more important)\n",
        "        if len(context.messages) > 0:\n",
        "            position = len(context.messages)\n",
        "            recency_boost = min(position / 10, 2.0)\n",
        "            score += recency_boost\n",
        "\n",
        "        return score\n",
        "\n",
        "    def add_message(self, memory: ConversationMemory, message: Dict[str, str]) -> ConversationMemory:\n",
        "        \"\"\"Add message with metadata tracking\"\"\"\n",
        "        memory.last_accessed = datetime.now()\n",
        "\n",
        "        # Calculate metadata\n",
        "        token_count = self.count_tokens(message.get(\"content\", \"\"))\n",
        "        importance_score = self.calculate_importance_score(message, memory)\n",
        "\n",
        "        metadata = MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=token_count,\n",
        "            importance_score=importance_score,\n",
        "            message_type=message.get(\"role\", \"normal\")\n",
        "        )\n",
        "\n",
        "        # Add to memory\n",
        "        memory.messages.append(message)\n",
        "        memory.metadata.append(metadata)\n",
        "        memory.total_tokens += token_count\n",
        "\n",
        "        # Apply context management strategy\n",
        "        return self._apply_context_strategy(memory)\n",
        "\n",
        "    def _apply_context_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Apply the selected context management strategy\"\"\"\n",
        "        if memory.total_tokens <= self.max_tokens:\n",
        "            return memory\n",
        "\n",
        "        if memory.context_strategy == ContextStrategy.SLIDING_WINDOW:\n",
        "            return self._sliding_window_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SUMMARIZATION:\n",
        "            return self._summarization_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SEMANTIC_COMPRESSION:\n",
        "            return self._semantic_compression_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.HIERARCHICAL:\n",
        "            return self._hierarchical_strategy(memory)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _sliding_window_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove oldest messages while preserving system messages\"\"\"\n",
        "        while memory.total_tokens > self.max_tokens and len(memory.messages) > 1:\n",
        "            # Find first non-system message to remove\n",
        "            for i, (msg, meta) in enumerate(zip(memory.messages, memory.metadata)):\n",
        "                if msg.get(\"role\") != \"system\":\n",
        "                    # Remove message and metadata\n",
        "                    removed_msg = memory.messages.pop(i)\n",
        "                    removed_meta = memory.metadata.pop(i)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "                    break\n",
        "            else:\n",
        "                # If only system messages left, remove oldest\n",
        "                if memory.messages:\n",
        "                    removed_msg = memory.messages.pop(0)\n",
        "                    removed_meta = memory.metadata.pop(0)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _summarization_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Summarize older messages when approaching token limit\"\"\"\n",
        "        if memory.total_tokens < self.summary_threshold:\n",
        "            return memory\n",
        "\n",
        "        # Find messages to summarize (older half)\n",
        "        mid_point = len(memory.messages) // 2\n",
        "        messages_to_summarize = memory.messages[:mid_point]\n",
        "\n",
        "        if len(messages_to_summarize) < 4:  # Need enough content to summarize\n",
        "            return self._sliding_window_strategy(memory)\n",
        "\n",
        "        # Create summary\n",
        "        summary_content = self._create_summary(messages_to_summarize)\n",
        "        summary_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"[SUMMARY] Previous conversation: {summary_content}\"\n",
        "        }\n",
        "\n",
        "        # Calculate tokens saved\n",
        "        tokens_to_remove = sum(meta.token_count for meta in memory.metadata[:mid_point])\n",
        "        summary_tokens = self.count_tokens(summary_message[\"content\"])\n",
        "\n",
        "        # Replace messages with summary\n",
        "        memory.messages = [summary_message] + memory.messages[mid_point:]\n",
        "        memory.metadata = [MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=summary_tokens,\n",
        "            importance_score=8.0,  # Summaries are important\n",
        "            message_type=\"summary\"\n",
        "        )] + memory.metadata[mid_point:]\n",
        "\n",
        "        memory.total_tokens = memory.total_tokens - tokens_to_remove + summary_tokens\n",
        "        memory.summaries.append(summary_message)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _create_summary(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"Create a summary of messages (simplified version)\"\"\"\n",
        "        # In production, you'd use an LLM for better summaries\n",
        "        content_parts = []\n",
        "        current_topic = None\n",
        "\n",
        "        for msg in messages:\n",
        "            role = msg.get(\"role\", \"\")\n",
        "            content = msg.get(\"content\", \"\")[:200]  # Truncate for summary\n",
        "\n",
        "            if role == \"user\":\n",
        "                content_parts.append(f\"User asked: {content}\")\n",
        "            elif role == \"assistant\":\n",
        "                content_parts.append(f\"Assistant responded: {content}\")\n",
        "\n",
        "        return \" | \".join(content_parts[-5:])  # Last 5 exchanges\n",
        "\n",
        "    def _semantic_compression_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove messages with lowest importance scores\"\"\"\n",
        "        if len(memory.messages) <= 2:  # Keep minimum viable conversation\n",
        "            return memory\n",
        "\n",
        "        # Sort by importance score (ascending)\n",
        "        indexed_items = list(enumerate(zip(memory.messages, memory.metadata)))\n",
        "        indexed_items.sort(key=lambda x: x[1][1].importance_score)\n",
        "\n",
        "        # Remove lowest importance messages until under token limit\n",
        "        to_remove = []\n",
        "        tokens_to_remove = 0\n",
        "\n",
        "        for idx, (msg, meta) in indexed_items:\n",
        "            if memory.total_tokens - tokens_to_remove <= self.max_tokens:\n",
        "                break\n",
        "            if msg.get(\"role\") != \"system\":  # Preserve system messages\n",
        "                to_remove.append(idx)\n",
        "                tokens_to_remove += meta.token_count\n",
        "\n",
        "        # Remove messages (in reverse order to maintain indices)\n",
        "        for idx in sorted(to_remove, reverse=True):\n",
        "            memory.messages.pop(idx)\n",
        "            removed_meta = memory.metadata.pop(idx)\n",
        "            memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _hierarchical_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Organize messages in hierarchical structure with different retention policies\"\"\"\n",
        "        # Implement tiered retention: recent (full), medium (compressed), old (summary)\n",
        "        now = datetime.now()\n",
        "\n",
        "        recent_threshold = timedelta(minutes=10)\n",
        "        medium_threshold = timedelta(hours=1)\n",
        "\n",
        "        recent_msgs = []\n",
        "        medium_msgs = []\n",
        "        old_msgs = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            age = now - meta.timestamp\n",
        "            if age <= recent_threshold:\n",
        "                recent_msgs.append((msg, meta))\n",
        "            elif age <= medium_threshold:\n",
        "                medium_msgs.append((msg, meta))\n",
        "            else:\n",
        "                old_msgs.append((msg, meta))\n",
        "\n",
        "        # Keep all recent, compress medium, summarize old\n",
        "        final_messages = []\n",
        "        final_metadata = []\n",
        "\n",
        "        # Add recent messages as-is\n",
        "        for msg, meta in recent_msgs:\n",
        "            final_messages.append(msg)\n",
        "            final_metadata.append(meta)\n",
        "\n",
        "        # Compress medium messages (every other message)\n",
        "        for i, (msg, meta) in enumerate(medium_msgs):\n",
        "            if i % 2 == 0 or msg.get(\"role\") == \"system\":\n",
        "                final_messages.append(msg)\n",
        "                final_metadata.append(meta)\n",
        "\n",
        "        # Summarize old messages\n",
        "        if old_msgs:\n",
        "            old_messages_only = [msg for msg, meta in old_msgs]\n",
        "            summary = self._create_summary(old_messages_only)\n",
        "            summary_msg = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"[ARCHIVED] Earlier conversation: {summary}\"\n",
        "            }\n",
        "            final_messages.insert(0, summary_msg)\n",
        "            final_metadata.insert(0, MessageMetadata(\n",
        "                timestamp=now,\n",
        "                token_count=self.count_tokens(summary_msg[\"content\"]),\n",
        "                importance_score=7.0,\n",
        "                message_type=\"archive\"\n",
        "            ))\n",
        "\n",
        "        # Update memory\n",
        "        memory.messages = final_messages\n",
        "        memory.metadata = final_metadata\n",
        "        memory.total_tokens = sum(meta.token_count for meta in final_metadata)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def get_context_stats(self, memory: ConversationMemory) -> Dict:\n",
        "        \"\"\"Get detailed context statistics\"\"\"\n",
        "        if not memory.messages:\n",
        "            return {\"total_messages\": 0, \"total_tokens\": 0}\n",
        "\n",
        "        message_types = {}\n",
        "        importance_distribution = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            msg_type = msg.get(\"role\", \"unknown\")\n",
        "            message_types[msg_type] = message_types.get(msg_type, 0) + 1\n",
        "            importance_distribution.append(meta.importance_score)\n",
        "\n",
        "        return {\n",
        "            \"total_messages\": len(memory.messages),\n",
        "            \"total_tokens\": memory.total_tokens,\n",
        "            \"token_utilization\": memory.total_tokens / self.max_tokens,\n",
        "            \"message_types\": message_types,\n",
        "            \"avg_importance\": sum(importance_distribution) / len(importance_distribution),\n",
        "            \"strategy\": memory.context_strategy.value,\n",
        "            \"summaries_created\": len(memory.summaries),\n",
        "            \"session_duration\": (memory.last_accessed - memory.created_at).total_seconds() / 60\n",
        "        }\n",
        "\n",
        "    def export_conversation(self, memory: ConversationMemory) -> str:\n",
        "        \"\"\"Export conversation for analysis or backup\"\"\"\n",
        "        export_data = {\n",
        "            \"conversation\": [\n",
        "                {\n",
        "                    \"message\": msg,\n",
        "                    \"metadata\": {\n",
        "                        \"timestamp\": meta.timestamp.isoformat(),\n",
        "                        \"token_count\": meta.token_count,\n",
        "                        \"importance_score\": meta.importance_score,\n",
        "                        \"message_type\": meta.message_type\n",
        "                    }\n",
        "                }\n",
        "                for msg, meta in zip(memory.messages, memory.metadata)\n",
        "            ],\n",
        "            \"summaries\": memory.summaries,\n",
        "            \"stats\": self.get_context_stats(memory),\n",
        "            \"persona\": memory.persona\n",
        "        }\n",
        "\n",
        "        return json.dumps(export_data, indent=2)\n",
        "\n",
        "# Testing and demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Test different strategies\n",
        "    strategies = [\n",
        "        ContextStrategy.SLIDING_WINDOW,\n",
        "        ContextStrategy.SUMMARIZATION,\n",
        "        ContextStrategy.SEMANTIC_COMPRESSION,\n",
        "        ContextStrategy.HIERARCHICAL\n",
        "    ]\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n=== Testing {strategy.value} ===\")\n",
        "\n",
        "        manager = AdvancedContextManager(max_tokens=50, strategy=strategy)\n",
        "        memory = ConversationMemory(context_strategy=strategy)\n",
        "\n",
        "        # Simulate conversation\n",
        "        test_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"},\n",
        "            {\"role\": \"user\", \"content\": \"Can you explain neural networks?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks...\"},\n",
        "            {\"role\": \"user\", \"content\": \"This is very important: I need to remember this for my exam tomorrow.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I'll help you remember the key concepts for your exam...\"},\n",
        "        ]\n",
        "\n",
        "        for msg in test_messages:\n",
        "            memory = manager.add_message(memory, msg)\n",
        "\n",
        "        stats = manager.get_context_stats(memory)\n",
        "        print(f\"Final stats: {stats}\")\n",
        "        print(f\"Messages retained: {len(memory.messages)}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing sliding_window ===\n",
            "Final stats: {'total_messages': 4, 'total_tokens': 45, 'token_utilization': 0.9, 'message_types': {'system': 1, 'assistant': 2, 'user': 1}, 'avg_importance': 8.18, 'strategy': 'sliding_window', 'summaries_created': 0, 'session_duration': 8.233333333333333e-06}\n",
            "Messages retained: 4\n",
            "\n",
            "=== Testing summarization ===\n",
            "Final stats: {'total_messages': 4, 'total_tokens': 45, 'token_utilization': 0.9, 'message_types': {'system': 1, 'assistant': 2, 'user': 1}, 'avg_importance': 8.18, 'strategy': 'summarization', 'summaries_created': 0, 'session_duration': 1.9e-06}\n",
            "Messages retained: 4\n",
            "\n",
            "=== Testing semantic_compression ===\n",
            "Final stats: {'total_messages': 5, 'total_tokens': 44, 'token_utilization': 0.88, 'message_types': {'system': 1, 'user': 3, 'assistant': 1}, 'avg_importance': 7.9, 'strategy': 'semantic_compression', 'summaries_created': 0, 'session_duration': 1.7e-06}\n",
            "Messages retained: 5\n",
            "\n",
            "=== Testing hierarchical ===\n",
            "Final stats: {'total_messages': 7, 'total_tokens': 65, 'token_utilization': 1.3, 'message_types': {'system': 1, 'user': 3, 'assistant': 3}, 'avg_importance': 6.795714285714287, 'strategy': 'hierarchical', 'summaries_created': 0, 'session_duration': 1.7833333333333333e-06}\n",
            "Messages retained: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51b1d08c"
      },
      "source": [
        "First, let's save the code from the previous cell into a Python file named `app.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0300d4f3",
        "outputId": "a89821ca-ba63-4081-bbc1-015ca92b4ada"
      },
      "source": [
        "%%writefile app.py\n",
        "# Copy and paste the complete code from the previous cell here\n",
        "import tiktoken\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import hashlib\n",
        "from enum import Enum\n",
        "\n",
        "class ContextStrategy(Enum):\n",
        "    SLIDING_WINDOW = \"sliding_window\"\n",
        "    SUMMARIZATION = \"summarization\"\n",
        "    SEMANTIC_COMPRESSION = \"semantic_compression\"\n",
        "    HIERARCHICAL = \"hierarchical\"\n",
        "\n",
        "@dataclass\n",
        "class MessageMetadata:\n",
        "    timestamp: datetime\n",
        "    token_count: int\n",
        "    importance_score: float = 0.0\n",
        "    message_type: str = \"normal\"  # system, user, assistant, summary\n",
        "    thread_id: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class ConversationMemory:\n",
        "    messages: List[Dict[str, str]] = field(default_factory=list)\n",
        "    metadata: List[MessageMetadata] = field(default_factory=list)\n",
        "    summaries: List[Dict[str, str]] = field(default_factory=list)\n",
        "    persona: str = \"assistant\"\n",
        "    total_tokens: int = 0\n",
        "    created_at: datetime = field(default_factory=datetime.now)\n",
        "    last_accessed: datetime = field(default_factory=datetime.now)\n",
        "    context_strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW\n",
        "\n",
        "class AdvancedContextManager:\n",
        "    def __init__(self, max_tokens: int = 4000, strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.strategy = strategy\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "        self.summary_threshold = max_tokens * 0.7  # Summarize when 70% full\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Count tokens in text\"\"\"\n",
        "        return len(self.encoding.encode(text))\n",
        "\n",
        "    def calculate_importance_score(self, message: Dict[str, str], context: ConversationMemory) -> float:\n",
        "        \"\"\"Calculate importance score for message prioritization\"\"\"\n",
        "        content = message.get(\"content\", \"\")\n",
        "        role = message.get(\"role\", \"\")\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        # Role-based scoring\n",
        "        if role == \"system\":\n",
        "            score += 10.0\n",
        "        elif role == \"user\":\n",
        "            score += 5.0\n",
        "        elif role == \"assistant\":\n",
        "            score += 3.0\n",
        "\n",
        "        # Content-based scoring\n",
        "        importance_keywords = [\n",
        "            \"important\", \"remember\", \"key\", \"critical\", \"essential\",\n",
        "            \"problem\", \"solution\", \"error\", \"bug\", \"issue\"\n",
        "        ]\n",
        "\n",
        "        for keyword in importance_keywords:\n",
        "            if keyword.lower() in content.lower():\n",
        "                score += 2.0\n",
        "\n",
        "        # Length-based scoring (longer messages often more important)\n",
        "        score += min(len(content) / 100, 5.0)\n",
        "\n",
        "        # Recency boost (recent messages slightly more important)\n",
        "        if len(context.messages) > 0:\n",
        "            position = len(context.messages)\n",
        "            recency_boost = min(position / 10, 2.0)\n",
        "            score += recency_boost\n",
        "\n",
        "        return score\n",
        "\n",
        "    def add_message(self, memory: ConversationMemory, message: Dict[str, str]) -> ConversationMemory:\n",
        "        \"\"\"Add message with metadata tracking\"\"\"\n",
        "        memory.last_accessed = datetime.now()\n",
        "\n",
        "        # Calculate metadata\n",
        "        token_count = self.count_tokens(message.get(\"content\", \"\"))\n",
        "        importance_score = self.calculate_importance_score(message, memory)\n",
        "\n",
        "        metadata = MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=token_count,\n",
        "            importance_score=importance_score,\n",
        "            message_type=message.get(\"role\", \"normal\")\n",
        "        )\n",
        "\n",
        "        # Add to memory\n",
        "        memory.messages.append(message)\n",
        "        memory.metadata.append(metadata)\n",
        "        memory.total_tokens += token_count\n",
        "\n",
        "        # Apply context management strategy\n",
        "        return self._apply_context_strategy(memory)\n",
        "\n",
        "    def _apply_context_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Apply the selected context management strategy\"\"\"\n",
        "        if memory.total_tokens <= self.max_tokens:\n",
        "            return memory\n",
        "\n",
        "        if memory.context_strategy == ContextStrategy.SLIDING_WINDOW:\n",
        "            return self._sliding_window_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SUMMARIZATION:\n",
        "            return self._summarization_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SEMANTIC_COMPRESSION:\n",
        "            return self._semantic_compression_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.HIERARCHICAL:\n",
        "            return self._hierarchical_strategy(memory)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _sliding_window_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove oldest messages while preserving system messages\"\"\"\n",
        "        while memory.total_tokens > self.max_tokens and len(memory.messages) > 1:\n",
        "            # Find first non-system message to remove\n",
        "            for i, (msg, meta) in enumerate(zip(memory.messages, memory.metadata)):\n",
        "                if msg.get(\"role\") != \"system\":\n",
        "                    # Remove message and metadata\n",
        "                    removed_msg = memory.messages.pop(i)\n",
        "                    removed_meta = memory.metadata.pop(i)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "                    break\n",
        "            else:\n",
        "                # If only system messages left, remove oldest\n",
        "                if memory.messages:\n",
        "                    removed_msg = memory.messages.pop(0)\n",
        "                    removed_meta = memory.metadata.pop(0)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _summarization_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Summarize older messages when approaching token limit\"\"\"\n",
        "        if memory.total_tokens < self.summary_threshold:\n",
        "            return memory\n",
        "\n",
        "        # Find messages to summarize (older half)\n",
        "        mid_point = len(memory.messages) // 2\n",
        "        messages_to_summarize = memory.messages[:mid_point]\n",
        "\n",
        "        if len(messages_to_summarize) < 4:  # Need enough content to summarize\n",
        "            return self._sliding_window_strategy(memory)\n",
        "\n",
        "        # Create summary\n",
        "        summary_content = self._create_summary(messages_to_summarize)\n",
        "        summary_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"[SUMMARY] Previous conversation: {summary_content}\"\n",
        "        }\n",
        "\n",
        "        # Calculate tokens saved\n",
        "        tokens_to_remove = sum(meta.token_count for meta in memory.metadata[:mid_point])\n",
        "        summary_tokens = self.count_tokens(summary_message[\"content\"])\n",
        "\n",
        "        # Replace messages with summary\n",
        "        memory.messages = [summary_message] + memory.messages[mid_point:]\n",
        "        memory.metadata = [MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=summary_tokens,\n",
        "            importance_score=8.0,  # Summaries are important\n",
        "            message_type=\"summary\"\n",
        "        )] + memory.metadata[mid_point:]\n",
        "\n",
        "        memory.total_tokens = memory.total_tokens - tokens_to_remove + summary_tokens\n",
        "        memory.summaries.append(summary_message)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _create_summary(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"Create a summary of messages (simplified version)\"\"\"\n",
        "        # In production, you'd use an LLM for better summaries\n",
        "        content_parts = []\n",
        "        current_topic = None\n",
        "\n",
        "        for msg in messages:\n",
        "            role = msg.get(\"role\", \"\")\n",
        "            content = msg.get(\"content\", \"\")[:200]  # Truncate for summary\n",
        "\n",
        "            if role == \"user\":\n",
        "                content_parts.append(f\"User asked: {content}\")\n",
        "            elif role == \"assistant\":\n",
        "                content_parts.append(f\"Assistant responded: {content}\")\n",
        "\n",
        "        return \" | \".join(content_parts[-5:])  # Last 5 exchanges\n",
        "\n",
        "    def _semantic_compression_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove messages with lowest importance scores\"\"\"\n",
        "        if len(memory.messages) <= 2:  # Keep minimum viable conversation\n",
        "            return memory\n",
        "\n",
        "        # Sort by importance score (ascending)\n",
        "        indexed_items = list(enumerate(zip(memory.messages, memory.metadata)))\n",
        "        indexed_items.sort(key=lambda x: x[1][1].importance_score)\n",
        "\n",
        "        # Remove lowest importance messages until under token limit\n",
        "        to_remove = []\n",
        "        tokens_to_remove = 0\n",
        "\n",
        "        for idx, (msg, meta) in indexed_items:\n",
        "            if memory.total_tokens - tokens_to_remove <= self.max_tokens:\n",
        "                break\n",
        "            if msg.get(\"role\") != \"system\":  # Preserve system messages\n",
        "                to_remove.append(idx)\n",
        "                tokens_to_remove += meta.token_count\n",
        "\n",
        "        # Remove messages (in reverse order to maintain indices)\n",
        "        for idx in sorted(to_remove, reverse=True):\n",
        "            memory.messages.pop(idx)\n",
        "            removed_meta = memory.metadata.pop(idx)\n",
        "            memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _hierarchical_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Organize messages in hierarchical structure with different retention policies\"\"\"\n",
        "        # Implement tiered retention: recent (full), medium (compressed), old (summary)\n",
        "        now = datetime.now()\n",
        "\n",
        "        recent_threshold = timedelta(minutes=10)\n",
        "        medium_threshold = timedelta(hours=1)\n",
        "\n",
        "        recent_msgs = []\n",
        "        medium_msgs = []\n",
        "        old_msgs = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            age = now - meta.timestamp\n",
        "            if age <= recent_threshold:\n",
        "                recent_msgs.append((msg, meta))\n",
        "            elif age <= medium_threshold:\n",
        "                medium_msgs.append((msg, meta))\n",
        "            else:\n",
        "                old_msgs.append((msg, meta))\n",
        "\n",
        "        # Keep all recent, compress medium, summarize old\n",
        "        final_messages = []\n",
        "        final_metadata = []\n",
        "\n",
        "        # Add recent messages as-is\n",
        "        for msg, meta in recent_msgs:\n",
        "            final_messages.append(msg)\n",
        "            final_metadata.append(meta)\n",
        "\n",
        "        # Compress medium messages (every other message)\n",
        "        for i, (msg, meta) in enumerate(medium_msgs):\n",
        "            if i % 2 == 0 or msg.get(\"role\") == \"system\":\n",
        "                final_messages.append(msg)\n",
        "                final_metadata.append(meta)\n",
        "\n",
        "        # Summarize old messages\n",
        "        if old_msgs:\n",
        "            old_messages_only = [msg for msg, meta in old_msgs]\n",
        "            summary = self._create_summary(old_messages_only)\n",
        "            summary_msg = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"[ARCHIVED] Earlier conversation: {summary}\"\n",
        "            }\n",
        "            final_messages.insert(0, summary_msg)\n",
        "            final_metadata.insert(0, MessageMetadata(\n",
        "                timestamp=now,\n",
        "                token_count=self.count_tokens(summary_msg[\"content\"]),\n",
        "                importance_score=7.0,\n",
        "                message_type=\"archive\"\n",
        "            ))\n",
        "\n",
        "        # Update memory\n",
        "        memory.messages = final_messages\n",
        "        memory.metadata = final_metadata\n",
        "        memory.total_tokens = sum(meta.token_count for meta in final_metadata)\n",
        "\n",
        "        return memory # Corrected: return the memory object\n",
        "\n",
        "    def get_context_stats(self, memory: ConversationMemory) -> Dict:\n",
        "        \"\"\"Get detailed context statistics\"\"\"\n",
        "        if not memory.messages:\n",
        "            return {\"total_messages\": 0, \"total_tokens\": 0}\n",
        "\n",
        "        message_types = {}\n",
        "        importance_distribution = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            msg_type = msg.get(\"role\", \"unknown\")\n",
        "            message_types[msg_type] = message_types.get(msg_type, 0) + 1\n",
        "            importance_distribution.append(meta.importance_score)\n",
        "\n",
        "        return {\n",
        "            \"total_messages\": len(memory.messages),\n",
        "            \"total_tokens\": memory.total_tokens,\n",
        "            \"token_utilization\": memory.total_tokens / self.max_tokens,\n",
        "            \"message_types\": message_types,\n",
        "            \"avg_importance\": sum(importance_distribution) / len(importance_distribution),\n",
        "            \"strategy\": memory.context_strategy.value,\n",
        "            \"summaries_created\": len(memory.summaries),\n",
        "            \"session_duration\": (memory.last_accessed - memory.created_at).total_seconds() / 60\n",
        "        }\n",
        "\n",
        "    def export_conversation(self, memory: ConversationMemory) -> str:\n",
        "        \"\"\"Export conversation for analysis or backup\"\"\"\n",
        "        export_data = {\n",
        "            \"conversation\": [\n",
        "                {\n",
        "                    \"message\": msg,\n",
        "                    \"metadata\": {\n",
        "                        \"timestamp\": meta.timestamp.isoformat(),\n",
        "                        \"token_count\": meta.token_count,\n",
        "                        \"importance_score\": meta.importance_score,\n",
        "                        \"message_type\": meta.message_type\n",
        "                    }\n",
        "                }\n",
        "                for msg, meta in zip(memory.messages, memory.metadata)\n",
        "            ],\n",
        "            \"summaries\": memory.summaries,\n",
        "            \"stats\": self.get_context_stats(memory),\n",
        "            \"persona\": memory.persona\n",
        "        }\n",
        "\n",
        "        return json.dumps(export_data, indent=2)\n",
        "\n",
        "# Testing and demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Test different strategies\n",
        "    strategies = [\n",
        "        ContextStrategy.SLIDING_WINDOW,\n",
        "        ContextStrategy.SUMMARIZATION,\n",
        "        ContextStrategy.SEMANTIC_COMPRESSION,\n",
        "        ContextStrategy.HIERARCHICAL\n",
        "    ]\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n=== Testing {strategy.value} ===\")\n",
        "\n",
        "        manager = AdvancedContextManager(max_tokens=50, strategy=strategy)\n",
        "        memory = ConversationMemory(context_strategy=strategy)\n",
        "\n",
        "        # Simulate conversation\n",
        "        test_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"},\n",
        "            {\"role\": \"user\", \"content\": \"Can you explain neural networks?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks...\"},\n",
        "            {\"role\": \"user\", \"content\": \"This is very important: I need to remember this for my exam tomorrow.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I'll help you remember the key concepts for your exam...\"},\n",
        "        ]\n",
        "\n",
        "        for msg in test_messages:\n",
        "            memory = manager.add_message(memory, msg)\n",
        "\n",
        "        stats = manager.get_context_stats(memory)\n",
        "        print(f\"Final stats: {stats}\")\n",
        "        print(f\"Messages retained: {len(memory.messages)}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5e32696"
      },
      "source": [
        "Now, install `ngrok` and run the Streamlit app. You'll get a public URL to access the app."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "b2ab1d61",
        "outputId": "8b533ec5-24b1-40c6-997e-a28ecbc5dfeb"
      },
      "source": [
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if any\n",
        "ngrok.kill()\n",
        "\n",
        "# Set your authtoken (replace with your actual ngrok authtoken)\n",
        "# You can get one from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "# You can also add it to Colab secrets named 'NGROK_AUTH_TOKEN'\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "\n",
        "# Open a tunnel to the Streamlit port (default is 8501)\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(f\"Streamlit App URL: {public_url}\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-27T20:46:27+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-27T20:46:27+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-41-740966420.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Open a tunnel to the Streamlit port (default is 8501)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpublic_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"8501\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"http\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Streamlit App URL: {public_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Opening tunnel named: {name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    448\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb0fc420"
      },
      "source": [
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}