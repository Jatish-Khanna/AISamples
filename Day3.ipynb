{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOnnCEyzb7QG4jG6uJ0Xmn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatish-Khanna/AISamples/blob/main/Day3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6e750b",
        "outputId": "796b0318-5690-4e6d-db6e-e04aae89f936"
      },
      "source": [
        "import tiktoken\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import hashlib\n",
        "from enum import Enum\n",
        "\n",
        "class ContextStrategy(Enum):\n",
        "    SLIDING_WINDOW = \"sliding_window\"\n",
        "    SUMMARIZATION = \"summarization\"\n",
        "    SEMANTIC_COMPRESSION = \"semantic_compression\"\n",
        "    HIERARCHICAL = \"hierarchical\"\n",
        "\n",
        "@dataclass\n",
        "class MessageMetadata:\n",
        "    timestamp: datetime\n",
        "    token_count: int\n",
        "    importance_score: float = 0.0\n",
        "    message_type: str = \"normal\"  # system, user, assistant, summary\n",
        "    thread_id: Optional[str] = None\n",
        "\n",
        "@dataclass\n",
        "class ConversationMemory:\n",
        "    messages: List[Dict[str, str]] = field(default_factory=list)\n",
        "    metadata: List[MessageMetadata] = field(default_factory=list)\n",
        "    summaries: List[Dict[str, str]] = field(default_factory=list)\n",
        "    persona: str = \"assistant\"\n",
        "    total_tokens: int = 0\n",
        "    created_at: datetime = field(default_factory=datetime.now)\n",
        "    last_accessed: datetime = field(default_factory=datetime.now)\n",
        "    context_strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW\n",
        "\n",
        "class AdvancedContextManager:\n",
        "    def __init__(self, max_tokens: int = 4000, strategy: ContextStrategy = ContextStrategy.SLIDING_WINDOW):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.strategy = strategy\n",
        "        self.encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "        self.summary_threshold = max_tokens * 0.7  # Summarize when 70% full\n",
        "\n",
        "    def count_tokens(self, text: str) -> int:\n",
        "        \"\"\"Count tokens in text\"\"\"\n",
        "        return len(self.encoding.encode(text))\n",
        "\n",
        "    def calculate_importance_score(self, message: Dict[str, str], context: ConversationMemory) -> float:\n",
        "        \"\"\"Calculate importance score for message prioritization\"\"\"\n",
        "        content = message.get(\"content\", \"\")\n",
        "        role = message.get(\"role\", \"\")\n",
        "\n",
        "        score = 0.0\n",
        "\n",
        "        # Role-based scoring\n",
        "        if role == \"system\":\n",
        "            score += 10.0\n",
        "        elif role == \"user\":\n",
        "            score += 5.0\n",
        "        elif role == \"assistant\":\n",
        "            score += 3.0\n",
        "\n",
        "        # Content-based scoring\n",
        "        importance_keywords = [\n",
        "            \"important\", \"remember\", \"key\", \"critical\", \"essential\",\n",
        "            \"problem\", \"solution\", \"error\", \"bug\", \"issue\"\n",
        "        ]\n",
        "\n",
        "        for keyword in importance_keywords:\n",
        "            if keyword.lower() in content.lower():\n",
        "                score += 2.0\n",
        "\n",
        "        # Length-based scoring (longer messages often more important)\n",
        "        score += min(len(content) / 100, 5.0)\n",
        "\n",
        "        # Recency boost (recent messages slightly more important)\n",
        "        if len(context.messages) > 0:\n",
        "            position = len(context.messages)\n",
        "            recency_boost = min(position / 10, 2.0)\n",
        "            score += recency_boost\n",
        "\n",
        "        return score\n",
        "\n",
        "    def add_message(self, memory: ConversationMemory, message: Dict[str, str]) -> ConversationMemory:\n",
        "        \"\"\"Add message with metadata tracking\"\"\"\n",
        "        memory.last_accessed = datetime.now()\n",
        "\n",
        "        # Calculate metadata\n",
        "        token_count = self.count_tokens(message.get(\"content\", \"\"))\n",
        "        importance_score = self.calculate_importance_score(message, memory)\n",
        "\n",
        "        metadata = MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=token_count,\n",
        "            importance_score=importance_score,\n",
        "            message_type=message.get(\"role\", \"normal\")\n",
        "        )\n",
        "\n",
        "        # Add to memory\n",
        "        memory.messages.append(message)\n",
        "        memory.metadata.append(metadata)\n",
        "        memory.total_tokens += token_count\n",
        "\n",
        "        # Apply context management strategy\n",
        "        return self._apply_context_strategy(memory)\n",
        "\n",
        "    def _apply_context_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Apply the selected context management strategy\"\"\"\n",
        "        if memory.total_tokens <= self.max_tokens:\n",
        "            return memory\n",
        "\n",
        "        if memory.context_strategy == ContextStrategy.SLIDING_WINDOW:\n",
        "            return self._sliding_window_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SUMMARIZATION:\n",
        "            return self._summarization_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.SEMANTIC_COMPRESSION:\n",
        "            return self._semantic_compression_strategy(memory)\n",
        "        elif memory.context_strategy == ContextStrategy.HIERARCHICAL:\n",
        "            return self._hierarchical_strategy(memory)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _sliding_window_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove oldest messages while preserving system messages\"\"\"\n",
        "        while memory.total_tokens > self.max_tokens and len(memory.messages) > 1:\n",
        "            # Find first non-system message to remove\n",
        "            for i, (msg, meta) in enumerate(zip(memory.messages, memory.metadata)):\n",
        "                if msg.get(\"role\") != \"system\":\n",
        "                    # Remove message and metadata\n",
        "                    removed_msg = memory.messages.pop(i)\n",
        "                    removed_meta = memory.metadata.pop(i)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "                    break\n",
        "            else:\n",
        "                # If only system messages left, remove oldest\n",
        "                if memory.messages:\n",
        "                    removed_msg = memory.messages.pop(0)\n",
        "                    removed_meta = memory.metadata.pop(0)\n",
        "                    memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _summarization_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Summarize older messages when approaching token limit\"\"\"\n",
        "        if memory.total_tokens < self.summary_threshold:\n",
        "            return memory\n",
        "\n",
        "        # Find messages to summarize (older half)\n",
        "        mid_point = len(memory.messages) // 2\n",
        "        messages_to_summarize = memory.messages[:mid_point]\n",
        "\n",
        "        if len(messages_to_summarize) < 4:  # Need enough content to summarize\n",
        "            return self._sliding_window_strategy(memory)\n",
        "\n",
        "        # Create summary\n",
        "        summary_content = self._create_summary(messages_to_summarize)\n",
        "        summary_message = {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"[SUMMARY] Previous conversation: {summary_content}\"\n",
        "        }\n",
        "\n",
        "        # Calculate tokens saved\n",
        "        tokens_to_remove = sum(meta.token_count for meta in memory.metadata[:mid_point])\n",
        "        summary_tokens = self.count_tokens(summary_message[\"content\"])\n",
        "\n",
        "        # Replace messages with summary\n",
        "        memory.messages = [summary_message] + memory.messages[mid_point:]\n",
        "        memory.metadata = [MessageMetadata(\n",
        "            timestamp=datetime.now(),\n",
        "            token_count=summary_tokens,\n",
        "            importance_score=8.0,  # Summaries are important\n",
        "            message_type=\"summary\"\n",
        "        )] + memory.metadata[mid_point:]\n",
        "\n",
        "        memory.total_tokens = memory.total_tokens - tokens_to_remove + summary_tokens\n",
        "        memory.summaries.append(summary_message)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _create_summary(self, messages: List[Dict[str, str]]) -> str:\n",
        "        \"\"\"Create a summary of messages (simplified version)\"\"\"\n",
        "        # In production, you'd use an LLM for better summaries\n",
        "        content_parts = []\n",
        "        current_topic = None\n",
        "\n",
        "        for msg in messages:\n",
        "            role = msg.get(\"role\", \"\")\n",
        "            content = msg.get(\"content\", \"\")[:200]  # Truncate for summary\n",
        "\n",
        "            if role == \"user\":\n",
        "                content_parts.append(f\"User asked: {content}\")\n",
        "            elif role == \"assistant\":\n",
        "                content_parts.append(f\"Assistant responded: {content}\")\n",
        "\n",
        "        return \" | \".join(content_parts[-5:])  # Last 5 exchanges\n",
        "\n",
        "    def _semantic_compression_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Remove messages with lowest importance scores\"\"\"\n",
        "        if len(memory.messages) <= 2:  # Keep minimum viable conversation\n",
        "            return memory\n",
        "\n",
        "        # Sort by importance score (ascending)\n",
        "        indexed_items = list(enumerate(zip(memory.messages, memory.metadata)))\n",
        "        indexed_items.sort(key=lambda x: x[1][1].importance_score)\n",
        "\n",
        "        # Remove lowest importance messages until under token limit\n",
        "        to_remove = []\n",
        "        tokens_to_remove = 0\n",
        "\n",
        "        for idx, (msg, meta) in indexed_items:\n",
        "            if memory.total_tokens - tokens_to_remove <= self.max_tokens:\n",
        "                break\n",
        "            if msg.get(\"role\") != \"system\":  # Preserve system messages\n",
        "                to_remove.append(idx)\n",
        "                tokens_to_remove += meta.token_count\n",
        "\n",
        "        # Remove messages (in reverse order to maintain indices)\n",
        "        for idx in sorted(to_remove, reverse=True):\n",
        "            memory.messages.pop(idx)\n",
        "            removed_meta = memory.metadata.pop(idx)\n",
        "            memory.total_tokens -= removed_meta.token_count\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def _hierarchical_strategy(self, memory: ConversationMemory) -> ConversationMemory:\n",
        "        \"\"\"Organize messages in hierarchical structure with different retention policies\"\"\"\n",
        "        # Implement tiered retention: recent (full), medium (compressed), old (summary)\n",
        "        now = datetime.now()\n",
        "\n",
        "        recent_threshold = timedelta(minutes=10)\n",
        "        medium_threshold = timedelta(hours=1)\n",
        "\n",
        "        recent_msgs = []\n",
        "        medium_msgs = []\n",
        "        old_msgs = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            age = now - meta.timestamp\n",
        "            if age <= recent_threshold:\n",
        "                recent_msgs.append((msg, meta))\n",
        "            elif age <= medium_threshold:\n",
        "                medium_msgs.append((msg, meta))\n",
        "            else:\n",
        "                old_msgs.append((msg, meta))\n",
        "\n",
        "        # Keep all recent, compress medium, summarize old\n",
        "        final_messages = []\n",
        "        final_metadata = []\n",
        "\n",
        "        # Add recent messages as-is\n",
        "        for msg, meta in recent_msgs:\n",
        "            final_messages.append(msg)\n",
        "            final_metadata.append(meta)\n",
        "\n",
        "        # Compress medium messages (every other message)\n",
        "        for i, (msg, meta) in enumerate(medium_msgs):\n",
        "            if i % 2 == 0 or msg.get(\"role\") == \"system\":\n",
        "                final_messages.append(msg)\n",
        "                final_metadata.append(meta)\n",
        "\n",
        "        # Summarize old messages\n",
        "        if old_msgs:\n",
        "            old_messages_only = [msg for msg, meta in old_msgs]\n",
        "            summary = self._create_summary(old_messages_only)\n",
        "            summary_msg = {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"[ARCHIVED] Earlier conversation: {summary}\"\n",
        "            }\n",
        "            final_messages.insert(0, summary_msg)\n",
        "            final_metadata.insert(0, MessageMetadata(\n",
        "                timestamp=now,\n",
        "                token_count=self.count_tokens(summary_msg[\"content\"]),\n",
        "                importance_score=7.0,\n",
        "                message_type=\"archive\"\n",
        "            ))\n",
        "\n",
        "        # Update memory\n",
        "        memory.messages = final_messages\n",
        "        memory.metadata = final_metadata\n",
        "        memory.total_tokens = sum(meta.token_count for meta in final_metadata)\n",
        "\n",
        "        return memory\n",
        "\n",
        "    def get_context_stats(self, memory: ConversationMemory) -> Dict:\n",
        "        \"\"\"Get detailed context statistics\"\"\"\n",
        "        if not memory.messages:\n",
        "            return {\"total_messages\": 0, \"total_tokens\": 0}\n",
        "\n",
        "        message_types = {}\n",
        "        importance_distribution = []\n",
        "\n",
        "        for msg, meta in zip(memory.messages, memory.metadata):\n",
        "            msg_type = msg.get(\"role\", \"unknown\")\n",
        "            message_types[msg_type] = message_types.get(msg_type, 0) + 1\n",
        "            importance_distribution.append(meta.importance_score)\n",
        "\n",
        "        return {\n",
        "            \"total_messages\": len(memory.messages),\n",
        "            \"total_tokens\": memory.total_tokens,\n",
        "            \"token_utilization\": memory.total_tokens / self.max_tokens,\n",
        "            \"message_types\": message_types,\n",
        "            \"avg_importance\": sum(importance_distribution) / len(importance_distribution),\n",
        "            \"strategy\": memory.context_strategy.value,\n",
        "            \"summaries_created\": len(memory.summaries),\n",
        "            \"session_duration\": (memory.last_accessed - memory.created_at).total_seconds() / 60\n",
        "        }\n",
        "\n",
        "    def export_conversation(self, memory: ConversationMemory) -> str:\n",
        "        \"\"\"Export conversation for analysis or backup\"\"\"\n",
        "        export_data = {\n",
        "            \"conversation\": [\n",
        "                {\n",
        "                    \"message\": msg,\n",
        "                    \"metadata\": {\n",
        "                        \"timestamp\": meta.timestamp.isoformat(),\n",
        "                        \"token_count\": meta.token_count,\n",
        "                        \"importance_score\": meta.importance_score,\n",
        "                        \"message_type\": meta.message_type\n",
        "                    }\n",
        "                }\n",
        "                for msg, meta in zip(memory.messages, memory.metadata)\n",
        "            ],\n",
        "            \"summaries\": memory.summaries,\n",
        "            \"stats\": self.get_context_stats(memory),\n",
        "            \"persona\": memory.persona\n",
        "        }\n",
        "\n",
        "        return json.dumps(export_data, indent=2)\n",
        "\n",
        "# Testing and demonstration\n",
        "if __name__ == \"__main__\":\n",
        "    # Test different strategies\n",
        "    strategies = [\n",
        "        ContextStrategy.SLIDING_WINDOW,\n",
        "        ContextStrategy.SUMMARIZATION,\n",
        "        ContextStrategy.SEMANTIC_COMPRESSION,\n",
        "        ContextStrategy.HIERARCHICAL\n",
        "    ]\n",
        "\n",
        "    for strategy in strategies:\n",
        "        print(f\"\\n=== Testing {strategy.value} ===\")\n",
        "\n",
        "        manager = AdvancedContextManager(max_tokens=500, strategy=strategy)\n",
        "        memory = ConversationMemory(context_strategy=strategy)\n",
        "\n",
        "        # Simulate conversation\n",
        "        test_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"},\n",
        "            {\"role\": \"user\", \"content\": \"Can you explain neural networks?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Neural networks are computing systems inspired by biological neural networks...\"},\n",
        "            {\"role\": \"user\", \"content\": \"This is very important: I need to remember this for my exam tomorrow.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"I'll help you remember the key concepts for your exam...\"},\n",
        "        ]\n",
        "\n",
        "        for msg in test_messages:\n",
        "            memory = manager.add_message(memory, msg)\n",
        "\n",
        "        stats = manager.get_context_stats(memory)\n",
        "        print(f\"Final stats: {stats}\")\n",
        "        print(f\"Messages retained: {len(memory.messages)}\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Testing sliding_window ===\n",
            "Final stats: {'total_messages': 7, 'total_tokens': 65, 'token_utilization': 0.13, 'message_types': {'system': 1, 'user': 3, 'assistant': 3}, 'avg_importance': 6.795714285714287, 'strategy': 'sliding_window', 'summaries_created': 0, 'session_duration': 3.3e-05}\n",
            "Messages retained: 7\n",
            "\n",
            "=== Testing summarization ===\n",
            "Final stats: {'total_messages': 7, 'total_tokens': 65, 'token_utilization': 0.13, 'message_types': {'system': 1, 'user': 3, 'assistant': 3}, 'avg_importance': 6.795714285714287, 'strategy': 'summarization', 'summaries_created': 0, 'session_duration': 1.2833333333333333e-05}\n",
            "Messages retained: 7\n",
            "\n",
            "=== Testing semantic_compression ===\n",
            "Final stats: {'total_messages': 7, 'total_tokens': 65, 'token_utilization': 0.13, 'message_types': {'system': 1, 'user': 3, 'assistant': 3}, 'avg_importance': 6.795714285714287, 'strategy': 'semantic_compression', 'summaries_created': 0, 'session_duration': 6.316666666666667e-06}\n",
            "Messages retained: 7\n",
            "\n",
            "=== Testing hierarchical ===\n",
            "Final stats: {'total_messages': 7, 'total_tokens': 65, 'token_utilization': 0.13, 'message_types': {'system': 1, 'user': 3, 'assistant': 3}, 'avg_importance': 6.795714285714287, 'strategy': 'hierarchical', 'summaries_created': 0, 'session_duration': 1.085e-05}\n",
            "Messages retained: 7\n"
          ]
        }
      ]
    }
  ]
}